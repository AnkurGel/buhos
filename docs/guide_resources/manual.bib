@article{bowes2012a,
abstract = {Background: Systematic literature reviews are increasingly used in software engineering. Most systematic literature reviews require several hundred papers to be examined and assessed. This is not a trivial task and can be time consuming and error-prone. Aim: We present SLuRp - our open source web enabled database that supports the management of systematic literature reviews. Method: We describe the functionality of SLuRp and explain how it supports all phases in a systematic literature review. Results: We show how we used SLuRp in our SLR. We discuss how SLuRp enabled us to generate complex results in which we had confidence. Conclusions: SLuRp supports all phases of an SLR and enables reliable results to be generated. If we are to have confidence in the outcomes of SLRs it is essential that such automated systems are used.},
author = {Bowes, David and Hall, Tracy and Beecham, Sarah},
doi = {10.1145/2372233.2372243},
file = {:home/cdx/doctorado/0-tesis/articulo{\_}buhos/docs{\_}final/10.11452372233.2372243.pdf:pdf},
journal = {Proceedings of the 2nd international workshop on Evidential assessment of software technologies - EAST '12},
pages = {33--36},
title = {{SLuRp : A Tool to Help Large Complex Systematic Literature Reviews Deliver Valid and Rigorous Results}},
year = {2012}
}
@article{howard2016a,
abstract = {Background: There is growing interest in using machine learning approaches to priority rank studies and reduce human burden in screening literature when conducting systematic reviews. In addition, identifying addressable questions during the problem formulation phase of systematic review can be challenging, especially for topics having a large literature base. Here, we assess the performance of the SWIFT-Review priority ranking algorithm for identifying studies relevant to a given research question. We also explore the use of SWIFT-Review during problem formulation to identify, categorize, and visualize research areas that are data rich/data poor within a large literature corpus. Methods: Twenty case studies, including 15 public data sets, representing a range of complexity and size, were used to assess the priority ranking performance of SWIFT-Review. For each study, seed sets of manually annotated included and excluded titles and abstracts were used for machine training. The remaining references were then ranked for relevance using an algorithm that considers term frequency and latent Dirichlet allocation (LDA) topic modeling. This ranking was evaluated with respect to (1) the number of studies screened in order to identify 95 {\%} of known relevant studies and (2) the "Work Saved over Sampling" (WSS) performance metric. To assess SWIFT-Review for use in problem formulation, PubMed literature search results for 171 chemicals implicated as EDCs were uploaded into SWIFT-Review (264,588 studies) and categorized based on evidence stream and health outcome. Patterns of search results were surveyed and visualized using a variety of interactive graphics. Results: Compared with the reported performance of other tools using the same datasets, the SWIFT-Review ranking procedure obtained the highest scores on 11 out of 15 of the public datasets. Overall, these results suggest that using machine learning to triage documents for screening has the potential to save, on average, more than 50 {\%} of the screening effort ordinarily required when using un-ordered document lists. In addition, the tagging and annotation capabilities of SWIFT-Review can be useful during the activities of scoping and problem formulation. Conclusions: Text-mining and machine learning software such as SWIFT-Review can be valuable tools to reduce the human screening burden and assist in problem formulation. {\textcopyright} 2016 Howard et al.},
author = {Howard, B E and Phillips, J and Miller, K and Tandon, A and Mav, D and Shah, M R and Holmgren, S and Pelch, K E and Walker, V and Rooney, A A and Macleod, M and Shah, R R and Thayer, K},
doi = {10.1186/s13643-016-0263-z},
file = {:home/cdx/doctorado/0-tesis/articulo{\_}buhos/docs{\_}final/10.1186s13643-016-0263-z.pdf:pdf},
journal = {Systematic Reviews},
title = {{SWIFT-Review: A text-mining workbench for systematic review}},
volume = {5},
year = {2016}
}
@article{kiritchenko2010a,
abstract = {Background. Clinical trials are one of the most important sources of evidence for guiding evidence-based practice and the design of new trials. However, most of this information is available only in free text - e.g., in journal publications - which is labour intensive to process for systematic reviews, meta-analyses, and other evidence synthesis studies. This paper presents an automatic information extraction system, called ExaCT, that assists users with locating and extracting key trial characteristics (e.g., eligibility criteria, sample size, drug dosage, primary outcomes) from full-text journal articles reporting on randomized controlled trials (RCTs). Methods. ExaCT consists of two parts: an information extraction (IE) engine that searches the article for text fragments that best describe the trial characteristics, and a web browser-based user interface that allows human reviewers to assess and modify the suggested selections. The IE engine uses a statistical text classifier to locate those sentences that have the highest probability of describing a trial characteristic. Then, the IE engine's second stage applies simple rules to these sentences to extract text fragments containing the target answer. The same approach is used for all 21 trial characteristics selected for this study. Results. We evaluated ExaCT using 50 previously unseen articles describing RCTs. The text classifier (first stage) was able to recover 88{\%} of relevant sentences among its top five candidates (top5 recall) with the topmost candidate being relevant in 80{\%} of cases (top1 precision). Precision and recall of the extraction rules (second stage) were 93{\%} and 91{\%}, respectively. Together, the two stages of the extraction engine were able to provide (partially) correct solutions in 992 out of 1050 test tasks (94{\%}), with a majority of these (696) representing fully correct and complete answers. Conclusions. Our experiments confirmed the applicability and efficacy of ExaCT. Furthermore, they demonstrated that combining a statistical method with 'weak' extraction rules can identify a variety of study characteristics. The system is flexible and can be extended to handle other characteristics and document types (e.g., study protocols). {\textcopyright} 2010 Kiritchenko et al; licensee BioMed Central Ltd.},
author = {Kiritchenko, Svetlana and de Bruijn, Berry and Carini, Simona and Martin, Joel and Sim, Ida},
doi = {10.1186/1472-6947-10-56},
file = {:home/cdx/doctorado/0-tesis/articulo{\_}buhos/docs{\_}final/10.11861472-6947-10-56.pdf:pdf},
isbn = {1472-6947 (Electronic)$\backslash$r1472-6947 (Linking)},
issn = {14726947},
journal = {BMC Medical Informatics and Decision Making},
number = {1},
pmid = {20920176},
title = {{ExaCT: automatic extraction of clinical trial characteristics from journal publications}},
volume = {10},
year = {2010}
}
@article{TorresTorres2017,
abstract = {Background Systematic reviews are a key part of healthcare evaluation. They involve important painstaking but repetitive work. A major producer of systematic reviews, the Cochrane Collaboration, employs Review Manager (RevMan) programme—a software which assists reviewers and produces XML-structured files. This paper describes an add-on programme (RevManHAL) which helps auto-generate the abstract, results and discussion sections of RevMan-generated reviews in multiple languages. The paper also describes future developments for RevManHAL. Methods RevManHAL was created in Java using NetBeans by a programmer working full time for 2 months. Results The resulting open-source programme uses editable phrase banks to envelop text/numbers from within the prepared RevMan file in formatted readable text of a chosen language. In this way, considerable parts of the review's ‘abstract', ‘results' and ‘discussion' sections are created and a phrase added to ‘acknowledgements'. Conclusion RevManHAL's output needs to be checked by reviewers, but already, from our experience within the Cochrane Schizophrenia Group (200 maintained reviews, 900 reviewers), RevManHAL has saved much time which is better employed thinking about the meaning of the data rather than restating them. Many more functions will become possible as review writing becomes increasingly automated.},
author = {{Torres Torres}, Mercedes and Adams, Clive E.},
doi = {10.1186/s13643-017-0421-y},
file = {:home/cdx/doctorado/0-tesis/articulo{\_}buhos/docs{\_}final/10.1186s13643-017-0421-y.pdf:pdf},
issn = {20464053},
journal = {Systematic Reviews},
number = {1},
pages = {1--7},
publisher = {Systematic Reviews},
title = {{RevManHAL: Towards automatic text generation in systematic reviews}},
volume = {6},
year = {2017}
}
@article{Yu2008,
abstract = {BACKGROUND: Synthesis of data from published human genetic association studies is a critical step in the translation of human genome discoveries into health applications. Although genetic association studies account for a substantial proportion of the abstracts in PubMed, identifying them with standard queries is not always accurate or efficient. Further automating the literature-screening process can reduce the burden of a labor-intensive and time-consuming traditional literature search. The Support Vector Machine (SVM), a well-established machine learning technique, has been successful in classifying text, including biomedical literature. The GAPscreener, a free SVM-based software tool, can be used to assist in screening PubMed abstracts for human genetic association studies. RESULTS: The data source for this research was the HuGE Navigator, formerly known as the HuGE Pub Lit database. Weighted SVM feature selection based on a keyword list obtained by the two-way z score method demonstrated the best screening performance, achieving 97.5{\%} recall, 98.3{\%} specificity and 31.9{\%} precision in performance testing. Compared with the traditional screening process based on a complex PubMed query, the SVM tool reduced by about 90{\%} the number of abstracts requiring individual review by the database curator. The tool also ascertained 47 articles that were missed by the traditional literature screening process during the 4-week test period. We examined the literature on genetic associations with preterm birth as an example. Compared with the traditional, manual process, the GAPscreener both reduced effort and improved accuracy. CONCLUSION: GAPscreener is the first free SVM-based application available for screening the human genetic association literature in PubMed with high recall and specificity. The user-friendly graphical user interface makes this a practical, stand-alone application. The software can be downloaded at no charge.},
author = {Yu, Wei and Clyne, Melinda and Dolan, Siobhan M. and Yesupriya, Ajay and Wulf, Anja and Liu, Tiebin and Khoury, Muin J. and Gwinn, Marta},
doi = {10.1186/1471-2105-9-205},
file = {:home/cdx/doctorado/0-tesis/articulo{\_}buhos/docs{\_}final/10.11861471-2105-9-205.pdf:pdf},
isbn = {1471-2105},
issn = {14712105},
journal = {BMC Bioinformatics},
pages = {1--9},
pmid = {18430222},
title = {{GAPscreener: An automatic tool for screening human genetic association literature in PubMed using the support vector machine technique}},
volume = {9},
year = {2008}
}
@article{Greenhalgh2005,
abstract = {Objective To describe where papers come from in a systematic review of complex evidence. Method Audit of how the 495 primary sources for the review were originally identified. Results Only 30{\%} of sources were obtained from the protocol defined at the outset of the study (that is, from the database and hand searches). Fifty one per cent were identified by "snowballing" (such as pursuing references of references), and 24{\%} by personal knowledge or personal contacts. Conclusion Systematic reviews of complex evidence cannot rely solely on protocol-driven search strategies.},
archivePrefix = {arXiv},
arxivId = {331:1064-5},
author = {Greenhalgh, Trisha and Peacock, Richard},
doi = {10.1136/bmj.38636.593461.68},
eprint = {331:1064-5},
file = {:home/cdx/doctorado/0-tesis/articulo{\_}buhos/docs{\_}final/1064.full.pdf:pdf},
isbn = {1468-5833},
issn = {09598146},
journal = {British Medical Journal},
number = {7524},
pages = {1064--1065},
pmid = {16230312},
title = {{Effectiveness and efficiency of search methods in systematic reviews of complex evidence: Audit of primary sources}},
volume = {331},
year = {2005}
}
